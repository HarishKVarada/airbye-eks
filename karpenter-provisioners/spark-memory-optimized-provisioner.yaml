apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: spark-memory-optimized
  namespace: karpenter
spec:
  kubeletConfiguration:
    containerRuntime: containerd
#    podsPerCore: 2
#    maxPods: 20
  requirements:
    - key: "topology.kubernetes.io/zone"
      operator: In
      values: [${azs}b] # Update the correct region and zone
    - key: "karpenter.sh/capacity-type"
      operator: In
      values: ["spot", "on-demand"]
    - key: "karpenter.k8s.aws/instance-family"
      operator: In
      values: ["r5d"]
    - key: "karpenter.k8s.aws/instance-size"
      operator: In
      values: ["2xlarge", "4xlarge", "8xlarge", "12xlarge", "16xlarge", "24xlarge"]
#    - key: "node.kubernetes.io/instance-type" #If not included, all instance types are considered
#      operator: In
#      values: ["r5d.4xlarge","r5d.8xlarge","r5d.12xlarge"] # 2 NVMe disk
    - key: "kubernetes.io/arch"
      operator: In
      values: ["amd64"]
  limits:
    resources:
      cpu: 1000
  providerRef: # optional, recommended to use instead of `provider`
    name: spark-memory-optimized
  labels:
    type: karpenter
    provisioner: spark-memory-optimized
    NodeGroupType: SparkMemoryOptimized
  taints:
    - key: spark-memory-optimized
      value: 'true'
      effect: NoSchedule
  # If omitted, the feature is disabled, nodes will never scale down due to low utilization
  ttlSecondsAfterEmpty: 120
  # Enables consolidation which attempts to reduce cluster cost by both removing un-needed nodes and down-sizing those
  # that can't be removed.  Mutually exclusive with the ttlSecondsAfterEmpty parameter.
#  consolidation:
#    enabled: true
  # If omitted, the feature is disabled and nodes will never expire.  If set to less time than it requires for a node
  # to become ready, the node may expire before any pods successfully start.
#  ttlSecondsUntilExpired: 2592000 # 30 Days = 60 * 60 * 24 * 30 Seconds;

---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: spark-memory-optimized
  namespace: karpenter
spec:
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true
  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
    httpTokens: required
  subnetSelector:
    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes
  securityGroupSelector:                      # required, when not using launchTemplate
    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes
  #  instanceProfile: ""      # optional, if already set in controller args
  #RAID0 config example
  userData: |
    MIME-Version: 1.0
    Content-Type: multipart/mixed; boundary="BOUNDARY"

    --BOUNDARY
    Content-Type: text/x-shellscript; charset="us-ascii"

    cat <<-EOF > /etc/profile.d/bootstrap.sh
    #!/bin/sh

    # Configure NVMe volumes in RAID0 configuration
    # https://github.com/awslabs/amazon-eks-ami/blob/056e31f8c7477e893424abce468cb32bbcd1f079/files/bootstrap.sh#L35C121-L35C126
    # Mount will be: /mnt/k8s-disks
    export LOCAL_DISKS='raid0'
    EOF

    # Source extra environment variables in bootstrap script
    sed -i '/^set -o errexit/a\\nsource /etc/profile.d/bootstrap.sh' /etc/eks/bootstrap.sh

    --BOUNDARY--

  tags:
    InstanceType: "spark-memory-optimized"    # optional, add tags for your own use
